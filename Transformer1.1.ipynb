{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load and inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Method Path  Query                                    Request Headers  \\\n",
      "0    GET    /  <EMP>  Host: 18.118.191.213\\nProxy-Connection: keep-a...   \n",
      "1    GET    /  <EMP>  Host: 16.182.101.101\\nProxy-Connection: keep-a...   \n",
      "2    GET    /  <EMP>  Host: 216.171.160.223\\nProxy-Connection: keep-...   \n",
      "3    GET    /  <EMP>  Host: 18.118.191.213\\nProxy-Connection: keep-a...   \n",
      "4    GET    /  <EMP>  Host: 16.182.101.101\\nProxy-Connection: keep-a...   \n",
      "\n",
      "                                        Request Body  Response Status  \\\n",
      "0  {\"source\": \"\", \"btnFlyerAction\": \"%00\", \"btnAc...              302   \n",
      "1         {\"searchQuery\": \"'; DROP TABLE users; --\"}              301   \n",
      "2  {\"_wpcf7\": \"admin'--\", \"_wpcf7_version\": \"\", \"...              301   \n",
      "3  {\"source\": \"1234567890\", \"btnFlyerAction\": \"%0...              302   \n",
      "4                     {\"searchQuery\": \"normalInput\"}              301   \n",
      "\n",
      "                                    Response Headers  \\\n",
      "0  Server: nginx/1.18.0 (Ubuntu)\\nDate: Thu, 22 A...   \n",
      "1  x-amz-error-code: WebsiteRedirect\\nx-amz-error...   \n",
      "2  Date: Thu, 22 Aug 2024 09:39:06 GMT\\nServer: A...   \n",
      "3  Server: nginx/1.18.0 (Ubuntu)\\nDate: Thu, 22 A...   \n",
      "4  x-amz-error-code: WebsiteRedirect\\nx-amz-error...   \n",
      "\n",
      "                                       Response Body  \n",
      "0  <html>\\r\\n<head><title>302 Found</title></head...  \n",
      "1  <html>\\n<head><title>301 Moved Permanently</ti...  \n",
      "2                                                NaN  \n",
      "3  <html>\\r\\n<head><title>302 Found</title></head...  \n",
      "4  <html>\\n<head><title>301 Moved Permanently</ti...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"combined_data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Inspect the first few rows of the data\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Extract the relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requests:\n",
      "   Method Path  Query                                    Request Headers\n",
      "0    GET    /  <EMP>  Host: 18.118.191.213\\nProxy-Connection: keep-a...\n",
      "1    GET    /  <EMP>  Host: 16.182.101.101\\nProxy-Connection: keep-a...\n",
      "2    GET    /  <EMP>  Host: 216.171.160.223\\nProxy-Connection: keep-...\n",
      "3    GET    /  <EMP>  Host: 18.118.191.213\\nProxy-Connection: keep-a...\n",
      "4    GET    /  <EMP>  Host: 16.182.101.101\\nProxy-Connection: keep-a...\n",
      "Responses:\n",
      "    Response Status                                   Response Headers  \\\n",
      "0              302  Server: nginx/1.18.0 (Ubuntu)\\nDate: Thu, 22 A...   \n",
      "1              301  x-amz-error-code: WebsiteRedirect\\nx-amz-error...   \n",
      "2              301  Date: Thu, 22 Aug 2024 09:39:06 GMT\\nServer: A...   \n",
      "3              302  Server: nginx/1.18.0 (Ubuntu)\\nDate: Thu, 22 A...   \n",
      "4              301  x-amz-error-code: WebsiteRedirect\\nx-amz-error...   \n",
      "\n",
      "                                       Response Body  \n",
      "0  <html>\\r\\n<head><title>302 Found</title></head...  \n",
      "1  <html>\\n<head><title>301 Moved Permanently</ti...  \n",
      "2                                                NaN  \n",
      "3  <html>\\r\\n<head><title>302 Found</title></head...  \n",
      "4  <html>\\n<head><title>301 Moved Permanently</ti...  \n"
     ]
    }
   ],
   "source": [
    "# Separate the request and response columns\n",
    "requests = data[['Method', 'Path', 'Query', 'Request Headers']]\n",
    "responses = data[['Response Status', 'Response Headers', 'Response Body']]\n",
    "\n",
    "# Inspect the first few rows of requests and responses\n",
    "print(\"Requests:\\n\", requests.head())\n",
    "print(\"Responses:\\n\", responses.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    # Convert to lowercase\n",
    "    # text = text.lower()\n",
    "\n",
    "    # Replace or normalize specific patterns\n",
    "    text = re.sub(r'\\n', ' ', text)  # Replace newlines with spaces\n",
    "\n",
    "    # Tokenize by splitting on spaces, slashes, commas, and colons\n",
    "    tokens = re.split(r'[ ,/:]+', text)  # Split by space, comma, slash, or colon\n",
    "\n",
    "    # Remove empty tokens (if any)\n",
    "    tokens = [token for token in tokens if token]\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Requests:\n",
      " ['GET', '<EMP>', 'Host', '18.118.191.213', 'Proxy-Connection', 'keep-alive', 'Upgrade-Insecure-Requests', '1', 'User-Agent', 'Mozilla', '5.0', '(Macintosh;', 'Intel', 'Mac', 'OS', 'X', '10_15_7)', 'AppleWebKit', '537.36', '(KHTML', 'like', 'Gecko)', 'Chrome', '127.0.0.0', 'Safari', '537.36', 'Accept', 'text', 'html', 'application', 'xhtml+xml', 'application', 'xml;q=0.9', 'image', 'avif', 'image', 'webp', 'image', 'apng', '*', '*;q=0.8', 'application', 'signed-exchange;v=b3;q=0.7', 'Accept-Encoding', 'gzip', 'deflate', 'Accept-Language', 'en-GB', 'en-US;q=0.9', 'en;q=0.8']\n",
      "Tokenized Responses:\n",
      " ['301', 'x-amz-error-code', 'WebsiteRedirect', 'x-amz-error-message', 'Request', 'does', 'not', 'contain', 'a', 'bucket', 'name.', 'x-amz-request-id', '7G7YJ115EK8TM5KM', 'x-amz-id-2', 'kkXjU5v8vvjazOiNuWOBHEAViLoy5WWSfBbHj5VpyZr5G5UBqZhW', 'dr+hXGsE2d3eaj7znOlLKI=', 'Location', 'https', 'aws.amazon.com', 's3', 'Content-Type', 'text', 'html;', 'charset=utf-8', 'Date', 'Thu', '22', 'Aug', '2024', '09', '38', '17', 'GMT', 'Server', 'AmazonS3', 'Content-Length', '348', '<html>', '<head><title>301', 'Moved', 'Permanently<', 'title><', 'head>', '<body>', '<h1>301', 'Moved', 'Permanently<', 'h1>', '<ul>', '<li>Code', 'WebsiteRedirect<', 'li>', '<li>Message', 'Request', 'does', 'not', 'contain', 'a', 'bucket', 'name.<', 'li>', '<li>RequestId', '7G7YJ115EK8TM5KM<', 'li>', '<li>HostId', 'kkXjU5v8vvjazOiNuWOBHEAViLoy5WWSfBbHj5VpyZr5G5UBqZhW', 'dr+hXGsE2d3eaj7znOlLKI=<', 'li>', '<', 'ul>', '<hr', '>', '<', 'body>', '<', 'html>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\AppData\\Local\\Temp\\ipykernel_63348\\1256235628.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  requests['tokenized'] = requests.apply(lambda row: tokenize(\" \".join(row.dropna().astype(str))), axis=1)\n",
      "C:\\Users\\rahul\\AppData\\Local\\Temp\\ipykernel_63348\\1256235628.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  responses['tokenized'] = responses.apply(lambda row: tokenize(\" \".join(row.dropna().astype(str))), axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Apply enhanced tokenization to the request and response components\n",
    "requests['tokenized'] = requests.apply(lambda row: tokenize(\" \".join(row.dropna().astype(str))), axis=1)\n",
    "responses['tokenized'] = responses.apply(lambda row: tokenize(\" \".join(row.dropna().astype(str))), axis=1)\n",
    "\n",
    "# Inspect the tokenized data\n",
    "print(\"Tokenized Requests:\\n\", requests['tokenized'][0])\n",
    "print(\"Tokenized Responses:\\n\", responses['tokenized'][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Vocabulary Size: 429\n",
      "Response Vocabulary Size: 10004\n",
      "{'537.36': 4, '': 5, '171.237.155.62': 6, 'Mozilla': 7, '5.0': 8, 'Macintosh': 9, 'Intel': 10, 'Mac': 11, 'OS': 12, 'X': 13, '10_15_7': 14, 'AppleWebKit': 15, 'KHTML': 16, 'like': 17, 'Gecko': 18, 'Chrome': 19, '127.0.0.0': 20, 'Safari': 21, 'gzip': 22, 'deflate': 23, 'enGB': 24, 'enUSq0.9': 25, 'enq0.8': 26, 'Host': 27, 'ProxyConnection': 28, 'keepalive': 29, 'UserAgent': 30, 'Accept': 31, 'AcceptEncoding': 32, 'AcceptLanguage': 33, 'GET': 34, 'http': 35, 'Referer': 36, '75.186.51.109': 37, '5000': 38, 'image': 39, 'css': 40, 'text': 41, 'Cookie': 42, 'secure': 43, 'EMP': 44, 'application': 45, 'q0.1': 46, 'IfModifiedSince': 47, 'Fri': 48, 'GMT': 49, 'IfNoneMatch': 50, '31': 51, 'Jul': 52, '2020': 53, '08': 54, '41': 55, '59': 56, '1596184919': 57, 'jsBase': 58, 'versionWebVersion': 59, 'webman': 60, 'style.css': 61, 'modules': 62, 'widget': 63, 'js': 64, 'avif': 65, 'webp': 66, 'apng': 67, 'q0.8': 68, 'WebVersionWebVersion': 69, 'jsCore': 70, 'lib': 71, 'module': 72, 'v1557438612': 73, 'javascript': 74, '1': 75, 'XRequestedWith': 76, 'XMLHttpRequest': 77, 'html': 78, 'xhtmlxml': 79, 'xmlq0.9': 80, 'signedexchangevb3q0.7': 81, 'q0.01': 82, 'UpgradeInsecureRequests': 83, 'svgxml': 84, 'scripts': 85, 'resources': 86, 'json': 87, 'ext3': 88, 'version2.400': 89, 'POST': 90, 'ContentLength': 91, 'Origin': 92, 'v1557438416': 93, 'common': 94, 'images': 95, 'ContentType': 96, 'xwwwformurlencoded': 97, 'skin.cssWebVersionWebVersion': 98, 'webapi': 99, 'charsetUTF8': 100, '0': 101, 'skin.css': 102, 'ui.css': 103, 'preLanguage': 104, 'common.js': 105, 'base64.js': 106, 'ecmascript': 107, 'xecmascript': 108, 'default': 109, 'ux': 110, 'web_caps': 111, 'entry.cgi': 112, 'v1557438613': 113, 'OutsideCmd': 114, '2x': 115, 'desktop.cssv1562143389': 116, '18.118.191.213': 117, '16.182.101.101': 118, '216.171.160.223': 119, '217.69.139.87': 120, 'alarm.css': 121, 'custom.css': 122, 'fn.css': 123, 'main.css': 124, 'pictures.css': 125, 'playback.css': 126, 'reset.css': 127, 'resize.css': 128, 'set.css': 129, 'thermal.css': 130, 'current_config': 131, 'custom_lang': 132, 'English.txt': 133, 'Custom': 134, 'local.png': 135, 'plain': 136, '00': 137, 'favicon.ico': 138, 'bg.png': 139, 'index.js': 140, 'publicFunc.js': 141, 'timeZoneExcel.js': 142, 'extend.js': 143, 'rpcBase.js': 144, 'aes.js': 145, 'jquery.base64.js': 146, 'jquery.js': 147, 'jquery.pubsub.js': 148, 'm.js': 149, 'md5.js': 150, 'more.js': 151, 'rsa.js': 152, 'sea.js': 153, 'seajstext.js': 154, 'colorpicker.css': 155, 'dui.datepicker.js': 156, 'dui.dialog.js': 157, 'dui.ipfield.js': 158, 'dui.numberfield.js': 159, 'dui.pagination.js': 160, 'dui.password.js': 161, 'dui.tab.js': 162, 'dui.table.js': 163, 'dui.textfield.js': 164, 'dui.timefield.js': 165, 'dui.tip.js': 166, 'dui.validator.js': 167, 'jquery.ui.core.js': 168, 'jquery.ui.widget.js': 169, 'ability.js': 170, 'app.js': 171, 'h5player.js': 172, 'mToken.js': 173, 'mTokenBasicOper.js': 174, 'mTokenOperator.js': 175, 'pageBase.js': 176, 'plugin.js': 177, 'pluginCanvas.js': 178, 'rpc.js': 179, 'rpcLogin.js': 180, 'sm3babel.js': 181, 'USBKeyInfoMap.js': 182, 'audioPlayer.js': 183, 'ivs.js': 184, 'mp4remux.js': 185, 'playerControl.js': 186, 'public.js': 187, 'streamDrawer.js': 188, 'Sylvester.js': 189, 'videoMediaSource.js': 190, 'WebGLCanvas.js': 191, 'WebsocketServer.js': 192, 'workerManager.js': 193, 'xthemegray.cssv1557438416': 194, 'checkbox.png': 195, 'synoSDSjslib': 196, 'webCapsConfig': 197, '3rdparty': 198, 'v1557438423': 199, 'v1557438611': 200, 'v1557438637': 201, 'login': 202, '61': 203, '63': 204, '138.195.193.96': 205, '52.95.129.99': 206, 'page404http': 207, 'upgradeinsecurerequests': 208, 'useragent': 209, 'accept': 210, 'secfetchsite': 211, 'none': 212, 'secfetchmode': 213, 'navigate': 214, 'secfetchuser': 215, 'secfetchdest': 216, 'document': 217, 'secchua': 218, 'NotABrandv99': 219, 'Google': 220, 'Chromev127': 221, 'Chromiumv127': 222, 'secchuamobile': 223, 'secchuaplatform': 224, 'macOS': 225, 'acceptencoding': 226, 'br': 227, 'zstd': 228, 'acceptlanguage': 229, 'priority': 230, 'u0': 231, 'i': 232, 'cookie': 233, 'acta97cc2f6b8254418a0f64a3cafdb0971': 234, 'oidJmDj2TwbjzYMWTmJFnjR': 235, 'mrcuEA4966C720E7737303299713AB7A': 236, '16.182.103.133': 237, '101.100.239.75': 238, '52.95.129.143': 239, '134': 240, '822': 241, '2772': 242, 'b394': 243, '680': 244, '1d24': 245, '2fd': 246, '2e3': 247, 'c0e2': 248, '39f8': 249, 'd8c': 250, '8a4a': 251, 'version1724325390457': 252, 'version1724325407695': 253, '1596184920': 254, '1869': 255, '42': 256, 'icons.png': 257, 'lgbg.jpg': 258, 'loginlogodh.jpg': 259, 'version1724325392839': 260, 'version1724325409572': 261, '15ae': 262, '1287': 263, '3802': 264, '1e01': 265, '14c4f': 266, '3541': 267, '5ba': 268, '5a2': 269, '178cb': 270, '38c': 271, '_1724325389047': 272, '_1724325406157': 273, '10ce': 274, '_1724325389048': 275, '_1724325406158': 276, '2e1a': 277, '207b': 278, '7c8': 279, '1d98': 280, '237f': 281, '76eb': 282, '1e2e': 283, '1535': 284, 'a43': 285, 'af2': 286, 'bc0': 287, '3f9': 288, '96f': 289, '4dee': 290, '11de': 291, 'e78': 292, '20a': 293, 'cf2': 294, '4bcc': 295, '19a0': 296, '2f60': 297, '20fa': 298, '5c7': 299, '29ae': 300, '34da': 301, '6364': 302, '5539': 303, '14d7': 304, '7e1': 305, 'cbbc': 306, '177f4': 307, '6ee4': 308, '1332': 309, 'aeb': 310, '12eb': 311, '152c': 312, 'a29': 313, '16d0': 314, '70a': 315, '174e': 316, '1d5a': 317, '546b': 318, '2318': 319, '20ef': 320, '2d46': 321, '2b06': 322, 'adapter': 323, 'ext': 324, 'extbase.js': 325, 'v1557438415': 326, 'extall.js': 327, 'extall.css': 328, 'xthemegray.css': 329, 'form': 330, 'textbg.gif': 331, 'gray': 332, 'qtip': 333, 'tipanchorsprite.gif': 334, 'business': 335, 'Components': 336, 'v0511042019102297': 337, 'uxall.cssv1557438416': 338, 'uxall.css': 339, 'uxall.js': 340, 'prototype1.7.2': 341, 'prototype.js': 342, 'scrollbar': 343, 'flexcroll.js': 344, 'sds.css': 345, 'v1557438421': 346, 'sds.js': 347, 'v1566189023': 348, '610950620': 349, '21': 350, '12': 351, 'May': 352, '1989': 353, '04': 354, '30': 355, '20': 356, 'version2.4001724325392461': 357, 'version2.4001724325409203': 358, 'apiSYNO.Core.Desktop.Defsversion1methodgetjsv1572479023': 359, 'apiSYNO.Core.Desktop.JSUIStringversion1methodgetjslangenuv1572478789': 360, 'apiSYNO.Core.Desktop.UIStringversion1methodgetjslangenuv1572478790': 361, 'apiSYNO.Core.Desktop.SessionDataversion1methodgetjsSynoTokenv1557438608': 362, 'OAuthService': 363, 'v1550043740': 364, 'SynoFinder': 365, 'v1555311246': 366, 'desktop.js': 367, 'v1562143388': 368, 'v4399': 369, 'login_background_hd.jpg': 370, 'id1': 371, 'AdminCenter': 372, 'v1557438650': 373, 'AudioPlayer': 374, 'BandwidthControl': 375, 'C3': 376, 'ClipBoardJS': 377, 'ConfigBackup': 378, 'DataDrivenDocuments': 379, 'DiskMessageHandler': 380, 'DSMNotify': 381, 'ExternalDevices': 382, 'EzInternet': 383, 'FileBrowser': 384, 'v1555584060': 385, 'FileTaskMonitor': 386, 'v1555584024': 387, 'HelpBrowser': 388, 'HotkeyManager': 389, 'iSCSI': 390, 'LogCenter': 391, 'v1557438595': 392, 'MyDSCenter': 393, 'PersonalSettings': 394, 'PhotoViewer': 395, 'PkgManApp': 396, 'PollingTask': 397, 'ResourceMonitor': 398, 'SecurityScan': 399, 'Share': 400, 'StorageManager': 401, 'SupportForm': 402, 'SystemInfoApp': 403, 'TaskSchedulerUtils': 404, 'TaskSchedulerWidget': 405, 'ThumbConvertProgress': 406, 'Utils': 407, 'VideoPlayer2': 408, 'v1557438610': 409, 'WelcomeApp': 410, 'WelcomeTip': 411, 'Widgets': 412, 'desktop.css': 413, 'v1562143389': 414, '1x': 415, 'preview_bar_bg.png': 416, 'v5934v0733162019034097': 417, 'sprites6efaa92e9b.png': 418, 'security.cgi': 419, 'XSQLComment': 420, 'admin': 421, 'XNullByte': 422, 'query.cgi': 423, '50': 424, 'ControlPanel': 425, 'dsm.cgi': 426, 'CacheControl': 427, 'maxage0': 428, '<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_tokens(tokens):\n",
    "    # Keep periods and other specified punctuation in tokens\n",
    "    return [re.sub(r'[^\\w\\s.]', '', token) for token in tokens if token]\n",
    "\n",
    "\n",
    "def build_vocab(tokenized_data, max_vocab_size=10000):\n",
    "    vocab = Counter()\n",
    "    for tokens in tokenized_data:\n",
    "        clean_tokens_list = clean_tokens(tokens)\n",
    "        vocab.update(clean_tokens_list)\n",
    "    \n",
    "    # Keep only the most frequent `max_vocab_size` words\n",
    "    most_common = vocab.most_common(max_vocab_size)\n",
    "    vocab = {word: i+4 for i, (word, _) in enumerate(most_common)}\n",
    "    \n",
    "    # Add special tokens\n",
    "    vocab['<PAD>'] = 0\n",
    "    vocab['<UNK>'] = 1\n",
    "    vocab['<SOS>'] = 2\n",
    "    vocab['<EOS>'] = 3\n",
    "    return vocab\n",
    "\n",
    "# Build vocabulary with a limited size\n",
    "vocab_responses = build_vocab(responses['tokenized'], max_vocab_size=10000)  # Example size limit\n",
    "vocab_requests = build_vocab(requests['tokenized'], max_vocab_size=10000)\n",
    "print(\"Request Vocabulary Size:\", len(vocab_requests))\n",
    "print(\"Response Vocabulary Size:\", len(vocab_responses))\n",
    "\n",
    "print(vocab_requests)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Numericalize and pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([248, 73])\n",
      "torch.Size([248, 36023])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "# Numericalize the sequences using the separate vocabularies\n",
    "def numericalize(tokens, vocab):\n",
    "    return [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
    "\n",
    "numericalized_requests = requests['tokenized'].apply(lambda x: numericalize(x, vocab_requests))\n",
    "numericalized_responses = responses['tokenized'].apply(lambda x: numericalize(x, vocab_responses))\n",
    "\n",
    "# Add <SOS> and <EOS> tokens to the responses for sequence generation\n",
    "numericalized_responses = numericalized_responses.apply(lambda x: [vocab_responses['<SOS>']] + x + [vocab_responses['<EOS>']])\n",
    "\n",
    "# Pad the sequences\n",
    "padded_requests = pad_sequence([torch.tensor(seq) for seq in numericalized_requests],\n",
    "                               batch_first=True, padding_value=vocab_requests['<PAD>'])\n",
    "padded_responses = pad_sequence([torch.tensor(seq) for seq in numericalized_responses],\n",
    "                                batch_first=True, padding_value=vocab_responses['<PAD>'])\n",
    "\n",
    "# Inspect the padded sequences\n",
    "print(padded_requests.shape)\n",
    "print(padded_responses.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Create a dataset and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 73]) torch.Size([32, 36023])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HTTPDataset(Dataset):\n",
    "    def __init__(self, requests, responses):\n",
    "        self.requests = requests\n",
    "        self.responses = responses\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.requests)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.requests[idx], self.responses[idx]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = HTTPDataset(padded_requests, padded_responses)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Inspect the first batch\n",
    "for batch in dataloader:\n",
    "    requests_batch, responses_batch = batch\n",
    "    print(requests_batch.shape, responses_batch.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Define the Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (request_embedding): Embedding(429, 512)\n",
      "  (response_embedding): Embedding(10004, 512)\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (fc_out): Linear(in_features=512, out_features=10004, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size_request, vocab_size_response, embed_size, num_heads, num_encoder_layers, num_decoder_layers, dim_feedforward):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.request_embedding = nn.Embedding(vocab_size_request, embed_size)\n",
    "        self.response_embedding = nn.Embedding(vocab_size_response, embed_size)\n",
    "        self.transformer = nn.Transformer(embed_size, num_heads, num_encoder_layers, num_decoder_layers, dim_feedforward)\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size_response)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.request_embedding(src)\n",
    "        tgt = self.response_embedding(tgt)\n",
    "        src = src.permute(1, 0, 2)  # Transformer expects (seq_len, batch, embed_size)\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "        transformer_out = self.transformer(src, tgt)\n",
    "        out = self.fc_out(transformer_out)\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size_request = len(vocab_requests)\n",
    "vocab_size_response = len(vocab_responses)\n",
    "embed_size = 512\n",
    "num_heads = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dim_feedforward = 512\n",
    "\n",
    "# Initialize model\n",
    "model = TransformerModel(vocab_size_request, vocab_size_response, embed_size, num_heads, num_encoder_layers, num_decoder_layers, dim_feedforward)\n",
    "\n",
    "# Inspect model\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:120: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "C:\\Users\\rahul\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\amp\\autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 166090813952 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m tgt_output \u001b[38;5;241m=\u001b[39m responses_batch[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequests_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Reshape output to (batch_size, vocab_size_response, seq_len) for loss calculation\u001b[39;00m\n\u001b[0;32m     60\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (batch_size, vocab_size_response, seq_len)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[25], line 17\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[1;34m(self, src, tgt)\u001b[0m\n\u001b[0;32m     15\u001b[0m src \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Transformer expects (seq_len, batch, embed_size)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m tgt \u001b[38;5;241m=\u001b[39m tgt\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m transformer_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(transformer_out)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:146\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src, mask\u001b[38;5;241m=\u001b[39msrc_mask, src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask)\n\u001b[1;32m--> 146\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:369\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    366\u001b[0m output \u001b[38;5;241m=\u001b[39m tgt\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 369\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    375\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:716\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    714\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x))\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 716\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    717\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))\n\u001b[0;32m    718\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:725\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    723\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[0;32m    724\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 725\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    729\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\activation.py:1205\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1191\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1192\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1202\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1203\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1205\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1214\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\functional.py:5373\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   5370\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m   5371\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[1;32m-> 5373\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5374\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m tgt_len, embed_dim)\n\u001b[0;32m   5376\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 166090813952 bytes."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import gc\n",
    "\n",
    "# Assume the TransformerModel class is already defined\n",
    "# Also assume the dataset and vocabulary dictionaries (vocab_requests, vocab_responses) are already created\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 10\n",
    "batch_size = 8  # Reduced batch size to avoid memory issues\n",
    "embed_size = 256  # Reduced from 512\n",
    "num_heads = 4     # Reduced number of attention heads\n",
    "num_encoder_layers = 2  # Reduced number of layers\n",
    "num_decoder_layers = 2  # Reduced number of layers\n",
    "dim_feedforward = 256  # Reduced feedforward dimension\n",
    "\n",
    "# Initialize model\n",
    "vocab_size_request = len(vocab_requests)\n",
    "vocab_size_response = len(vocab_responses)\n",
    "model = TransformerModel(vocab_size_request, vocab_size_response, embed_size, num_heads, num_encoder_layers, num_decoder_layers, dim_feedforward)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab_responses['<PAD>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Mixed precision training scaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Define DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for requests_batch, responses_batch in train_loader:\n",
    "        requests_batch, responses_batch = requests_batch.to(device), responses_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Use autocast for mixed precision\n",
    "        with autocast():\n",
    "            # Prepare inputs and targets for the decoder\n",
    "            tgt_input = responses_batch[:, :-1]\n",
    "            tgt_output = responses_batch[:, 1:]\n",
    "\n",
    "            # Forward pass through the model\n",
    "            output = model(requests_batch, tgt_input)\n",
    "\n",
    "            # Reshape output to (batch_size, vocab_size_response, seq_len) for loss calculation\n",
    "            output = output.permute(1, 2, 0)  # (batch_size, vocab_size_response, seq_len)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output, tgt_output)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Clear GPU cache\n",
    "        del requests_batch, responses_batch, tgt_input, tgt_output, output\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Optionally, add validation and save checkpoints here\n",
    "\n",
    "# You can add validation code and model checkpointing if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, request_seq, max_len=100):\n",
    "    model.eval()\n",
    "    generated_response = [vocab_responses['<SOS>']]\n",
    "\n",
    "    request_seq = request_seq.unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        tgt_input = torch.tensor(generated_response).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(request_seq, tgt_input)\n",
    "\n",
    "        next_token = output.argmax(2)[:, -1].item()\n",
    "        generated_response.append(next_token)\n",
    "\n",
    "        if next_token == vocab_responses['<EOS>']:\n",
    "            break\n",
    "\n",
    "    generated_response = [list(vocab_responses.keys())[list(vocab_responses.values()).index(idx)] for idx in generated_response]\n",
    "    return generated_response\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
